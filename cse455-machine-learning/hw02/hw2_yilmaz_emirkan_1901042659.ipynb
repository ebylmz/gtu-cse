{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract"
      ],
      "metadata": {
        "id": "fHAfZZOSLJ6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The study aims to predict the age of abalone based on physical measurements, approaching it as a regression problem. The dataset provided comprises one categorical and seven numerical features, making a total of eight. The decision tree structure was designed to accommodate both categorical and numerical features. The optimal split was determined by experimenting with various threshold values and selecting the one that minimizes the mean square error (MSE). During the selection of the best split point, a method similar to what was taught in lectures was employed. Numerical values were sorted, and their midpoint was assessed for the minimum MSE.\n",
        "\n",
        "The decision tree structure was designed to be adaptable for use in a random forest by incorporating an additional parameter for random feature selection. This parameter ranges between 0 and 1, indicating the proportion of random features to be utilized in a node. Additionally, both pre-pruning and post-pruning techniques were implemented, though only post-pruning was utilized, as specified in the homework guidelines.\n",
        "\n",
        "The performance of the trained models was evaluated using k-fold cross-validation, with the results clearly favoring the random forest. The decision tree model reported a mean squared error (MSE) of 8.87, indicating less precision compared to the random forest model, which achieved a substantially lower MSE of 4.74. This difference can be attributed to the inherent design of the random forest approach, which aggregates predictions from a multitude of random decision trees, thus providing a more robust and generalized outcome than a single decision tree."
      ],
      "metadata": {
        "id": "YaSRGtgxLMSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "M1vjvo5MZp75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tZudUBD_k_c",
        "outputId": "e0f18111-be75-4a82-c710-751e8bb7f618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, KFold"
      ],
      "metadata": {
        "id": "cNekYNwfZpS9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "uNvB9lHHAQB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "from enum import Enum\n",
        "\n",
        "class FeatureType(Enum):\n",
        "    Numeric = 1\n",
        "    Categorical = 2\n",
        "\n",
        "def convert_feature_types(feature_types):\n",
        "    feature_t = []\n",
        "\n",
        "    for t in feature_types:\n",
        "        if t == 'Categorical':\n",
        "            feature_t.append(FeatureType.Categorical)\n",
        "        else:\n",
        "            feature_t.append(FeatureType.Numeric)\n",
        "    return feature_t\n",
        "\n",
        "# fetch dataset\n",
        "abalone = fetch_ucirepo(id=1)\n",
        "\n",
        "# data as pandas dataframes, convert to numpy\n",
        "X = abalone.data.features.values\n",
        "y = abalone.data.targets['Rings'].values\n",
        "\n",
        "feature_names = abalone.data.features.columns\n",
        "feature_types = convert_feature_types(abalone.variables['type'])\n",
        "\n",
        "# variable information\n",
        "print(abalone.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY3an0l5APUJ",
        "outputId": "f57704cc-ee02-4e19-f73d-2200adba7f4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             name     role         type demographic  \\\n",
            "0             Sex  Feature  Categorical        None   \n",
            "1          Length  Feature   Continuous        None   \n",
            "2        Diameter  Feature   Continuous        None   \n",
            "3          Height  Feature   Continuous        None   \n",
            "4    Whole_weight  Feature   Continuous        None   \n",
            "5  Shucked_weight  Feature   Continuous        None   \n",
            "6  Viscera_weight  Feature   Continuous        None   \n",
            "7    Shell_weight  Feature   Continuous        None   \n",
            "8           Rings   Target      Integer        None   \n",
            "\n",
            "                   description  units missing_values  \n",
            "0         M, F, and I (infant)   None             no  \n",
            "1    Longest shell measurement     mm             no  \n",
            "2      perpendicular to length     mm             no  \n",
            "3           with meat in shell     mm             no  \n",
            "4                whole abalone  grams             no  \n",
            "5               weight of meat  grams             no  \n",
            "6  gut weight (after bleeding)  grams             no  \n",
            "7            after being dried  grams             no  \n",
            "8  +1.5 gives the age in years   None             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Decision Tree"
      ],
      "metadata": {
        "id": "L2Uqzox4Azx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionNode:\n",
        "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_idx = feature_idx\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, r_features=1.0, min_samples_split=1, n_max_thresholds=32):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.r_features = r_features # Rate of features to be seen for each node (less than 1 for a Random Decition Tree)\n",
        "        self.root = None\n",
        "        self.feature_types = None\n",
        "        self.feature_names = None\n",
        "        self.n_features = 0\n",
        "        self.n_max_thresholds = n_max_thresholds # Maximum number of numerical thresholds to be tried for minimum MSE\n",
        "\n",
        "    def fit(self, X, y, feature_types, feature_names):\n",
        "        self.n_features = X.shape[1]\n",
        "        self.feature_types = feature_types\n",
        "        self.feature_names = feature_names\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _calculate_mse(self, left_y, right_y):\n",
        "        total_samples = len(left_y) + len(right_y)\n",
        "        left_mse = np.mean((left_y - np.mean(left_y))**2)\n",
        "        right_mse = np.mean((right_y - np.mean(right_y))**2)\n",
        "\n",
        "        return (len(left_y) / total_samples) * left_mse + (len(right_y) / total_samples) * right_mse\n",
        "\n",
        "    def _split_numeric(self, X, y, feature_index):\n",
        "        best_mse = np.inf\n",
        "        best_threshold = None\n",
        "\n",
        "        # Sort the column values and take the medium of two consecutive values (np.unique returns the ordered unique elements)\n",
        "        values = np.unique(X[:, feature_index])\n",
        "        thresholds = (values[1:] + values[:-1]) / 2\n",
        "\n",
        "        # Randomly sample potential thresholds, in case there are many possible splitting points\n",
        "        if len(thresholds) > self.n_max_thresholds:\n",
        "            thresholds = np.random.choice(thresholds, size=self.n_max_thresholds, replace=False)\n",
        "\n",
        "        for thresh in thresholds:\n",
        "            # Split the DataFrame according to the split value\n",
        "            left_indices = X[:, feature_index] > thresh\n",
        "            right_indices = ~left_indices\n",
        "\n",
        "            # If there is no information gain\n",
        "            if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the errors E(X - X_expected) for left and right splits\n",
        "            mse = self._calculate_mse(y[left_indices], y[right_indices])\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_threshold = thresh\n",
        "\n",
        "        return best_mse, best_threshold\n",
        "\n",
        "    def _split_categorical(self, X, y, feature_index):\n",
        "        best_mse = np.inf\n",
        "        best_threshold = None\n",
        "\n",
        "        categorical_values = np.unique(X[:, feature_index])\n",
        "        for thres in categorical_values:\n",
        "            left_indices = X[:, feature_index] == thres\n",
        "            right_indices = ~left_indices\n",
        "\n",
        "            # If there is no information gain\n",
        "            if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the errors E(X - X_expected) for left and right splits\n",
        "            mse = self._calculate_mse(y[left_indices], y[right_indices])\n",
        "\n",
        "            if mse < best_mse:\n",
        "                best_mse = mse\n",
        "                best_threshold = thres\n",
        "\n",
        "        return best_mse, best_threshold\n",
        "\n",
        "    def _get_features(self):\n",
        "        # Get all features\n",
        "        if self.r_features == 1.0:\n",
        "            features = [i for i in range(self.n_features)]\n",
        "        else: # self.r_features < 1.0\n",
        "            # Randomly select a subset of the features\n",
        "            n_random_features = max(1, round(self.n_features * self.r_features))\n",
        "            features = np.random.choice(self.n_features, size=n_random_features, replace=False)\n",
        "\n",
        "        return features\n",
        "\n",
        "    # Generated a decision tree recursively, in a greedy approach\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # In case of leaf node\n",
        "        if depth == self.max_depth or n_samples < self.min_samples_split:\n",
        "            return DecisionNode(value=np.mean(y))\n",
        "\n",
        "        features = self._get_features()\n",
        "        best_mse = np.inf\n",
        "        best_threshold = None\n",
        "        best_feature_idx = None\n",
        "\n",
        "        # Systematically iterate over the features\n",
        "        for feature_idx in features:\n",
        "            # Use '>' for numeric, '=' for categorical features\n",
        "            if self.feature_types[feature_idx] == FeatureType.Numeric:\n",
        "                mse, threshold = self._split_numeric(X, y, feature_idx)\n",
        "            else:\n",
        "                mse, threshold = self._split_categorical(X, y, feature_idx)\n",
        "\n",
        "            # Update the node, when a better split is found\n",
        "            if mse < best_mse:\n",
        "                best_threshold = threshold\n",
        "                best_mse = mse\n",
        "                best_feature_idx = feature_idx\n",
        "\n",
        "        if not best_threshold:\n",
        "            return DecisionNode(value=np.mean(y))\n",
        "\n",
        "        # Split the dataset according to threshold value\n",
        "        if self.feature_types[best_feature_idx] == FeatureType.Numeric:\n",
        "            left_indices = X[:, best_feature_idx] > best_threshold\n",
        "            right_indices = ~left_indices # X[feature] <= best_threshold\n",
        "        else:\n",
        "            left_indices = X[:, best_feature_idx] == best_threshold\n",
        "            right_indices = ~left_indices # X[feature] != best_threshold\n",
        "\n",
        "        left_tree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_tree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "\n",
        "        return DecisionNode(feature_idx=best_feature_idx, threshold=best_threshold, left=left_tree, right=right_tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _predict_tree(self, x, node):\n",
        "        # If the current node is a leaf, return its value as prediction\n",
        "        if node.value:\n",
        "            return node.value\n",
        "\n",
        "        # Otherwise, determine the prediction based on the node's split condition\n",
        "        test_value = x[node.feature_idx]\n",
        "\n",
        "        if self.feature_types[node.feature_idx] == FeatureType.Numeric:\n",
        "            child_node = node.left if (test_value > node.threshold) else node.right\n",
        "        else: # Feature is categorical\n",
        "            child_node = node.left if (test_value == node.threshold) else node.right\n",
        "\n",
        "        return self._predict_tree(x, child_node)\n",
        "\n",
        "    def prune(self, X_val, y_val):\n",
        "        self._prune_tree(self.root, X_val, y_val)\n",
        "\n",
        "    def _prune_tree(self, node, X_val, y_val):\n",
        "        # No pruning for a leaf node\n",
        "        if node.value:\n",
        "            return\n",
        "\n",
        "        # Prune children first\n",
        "        self._prune_tree(node.left, X_val, y_val)\n",
        "        self._prune_tree(node.right, X_val, y_val)\n",
        "\n",
        "        # Prune if both children are leaves\n",
        "        if node.left.value and node.right.value:\n",
        "            # Evaluate performance with and without the node\n",
        "            y_pred_before_prune = self.predict(X_val)\n",
        "\n",
        "            # Prune the subtree by making this node a leaf\n",
        "            node.value = np.mean([node.left.value, node.right.value])\n",
        "            y_pred_after_prune = self.predict(X_val)\n",
        "\n",
        "            # Compare performance before and after pruning\n",
        "            mse_before_prune = np.mean((y_val - y_pred_before_prune) ** 2)\n",
        "            mse_after_prune = np.mean((y_val - y_pred_after_prune) ** 2)\n",
        "\n",
        "            # If pruning improves performance, keep the pruned subtree\n",
        "            if mse_after_prune < mse_before_prune:\n",
        "                node.left = None\n",
        "                node.right = None\n",
        "            else:\n",
        "                node.value = None\n",
        "\n",
        "    def display(self):\n",
        "        self._display(self.root, depth=0)\n",
        "\n",
        "    def _display(self, node, depth):\n",
        "        margin = \" \" * (depth * 5)\n",
        "        if node.value:\n",
        "            print(f\"{margin}return [{node.value:.3f}]\")\n",
        "        else:\n",
        "            feature = self.feature_names[node.feature_idx] if self.feature_names.any() else node.feature_idx\n",
        "\n",
        "            if self.feature_types[node.feature_idx] == FeatureType.Numeric:\n",
        "                if_stmt = f\"if x['{feature}'] > {node.threshold:.3f}:\"\n",
        "                else_stmt = f\"else: # x['{feature}'] <= {node.threshold:.3f}\"\n",
        "            else:\n",
        "                if_stmt = f\"if x['{feature}'] == {node.threshold}:\"\n",
        "                else_stmt = f\"else: # x['{feature}'] != {node.threshold}\"\n",
        "\n",
        "            print(f\"{margin}{if_stmt}\")\n",
        "            self._display(node.left, depth + 1)\n",
        "\n",
        "            print(f\"{margin}{else_stmt}\")\n",
        "            self._display(node.right, depth + 1)"
      ],
      "metadata": {
        "id": "dZCvR38PA2Ph"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dt(X, y, feature_types, feature_names=None):\n",
        "    dt = DecisionTree()\n",
        "    dt.fit(X, y, feature_types, feature_names)\n",
        "    return dt\n",
        "\n",
        "def predict_dt(dt, X):\n",
        "    return dt.predict(X)"
      ],
      "metadata": {
        "id": "IyuLn5I_B-rE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Testing"
      ],
      "metadata": {
        "id": "rgyyp0Gvw17q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = build_dt(X_train, y_train, feature_types, feature_names)\n",
        "\n",
        "y_pred = predict_dt(dt, X_train)\n",
        "mse = mean_squared_error(y_train, y_pred)\n",
        "print(f\"MSE on training set: {mse:.2f}\")\n",
        "\n",
        "y_pred = predict_dt(dt, X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE on test set: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "t0eD1fvIwu_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "307639c2-8efb-4c44-c403-e890d2bda335"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE on training set: 0.00\n",
            "MSE on test set: 8.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MSE for the training set on a non-pruned decision tree is 0.00, indicating that the tree is overfitted, as expected."
      ],
      "metadata": {
        "id": "vOBgQY0KTJgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt.display()"
      ],
      "metadata": {
        "id": "xwOz34DSS2Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results of k-fold cross validation"
      ],
      "metadata": {
        "id": "8i2GO2sNaGz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform k-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "mse_scores = []\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    dt = build_dt(X_train, y_train, feature_types, feature_names)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = predict_dt(dt, X_val)\n",
        "\n",
        "    # Compute MSE\n",
        "    mse_fold = mean_squared_error(y_val, y_pred)\n",
        "    mse_scores.append(mse_fold)\n",
        "\n",
        "    print(f\"MSE for fold: {mse_fold:.2f}\")\n",
        "\n",
        "# Average MSE across all folds on training set\n",
        "avg_mse = sum(mse_scores) / len(mse_scores)\n",
        "print(f\"Average MSE from k-fold Cross-Validation: {avg_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbx1StJ9aKnf",
        "outputId": "2c319624-2b1d-4288-a766-37237b63d16f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for fold: 8.48\n",
            "MSE for fold: 8.52\n",
            "MSE for fold: 9.68\n",
            "MSE for fold: 9.49\n",
            "MSE for fold: 8.17\n",
            "Average MSE from k-fold Cross-Validation: 8.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Testing with Pruning"
      ],
      "metadata": {
        "id": "mULQkogzxFu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = build_dt(X_train, y_train, feature_types, feature_names)\n",
        "\n",
        "y_pred = predict_dt(dt, X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE: {mse:.2f}\")\n",
        "\n",
        "dt.prune(X_test, y_test)\n",
        "y_pred = predict_dt(dt, X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE (post-pruning): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "bqrGxXbKbORM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8e369d-ad2e-46d1-9116-3e5b4dff5e56"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 8.95\n",
            "MSE (post-pruning): 8.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying post-pruning, there is a slight decrease in MSE."
      ],
      "metadata": {
        "id": "EgTzTtb9T1uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of RDF"
      ],
      "metadata": {
        "id": "Sy3KaItbvEJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=None, r_features=0.3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.trees = []    # List of decision trees\n",
        "        self.max_depth = max_depth\n",
        "        self.r_features = r_features  # Rate of features to be seen on each node\n",
        "\n",
        "    def fit(self, X, y, feature_types, feature_names):\n",
        "        n_samples = X.shape[0]\n",
        "        self.trees = []\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            X_boot, y_boot = self._bootstrap(X, y)\n",
        "\n",
        "            dt = DecisionTree(max_depth=self.max_depth, r_features=self.r_features)\n",
        "            dt.fit(X_boot, y_boot, feature_types, feature_names)\n",
        "            self.trees.append(dt)\n",
        "\n",
        "    def _bootstrap(self, X, y):\n",
        "        num_samples = len(X)\n",
        "        indices = np.random.choice(len(X), size=num_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.zeros(len(X))\n",
        "\n",
        "        for tree in self.trees:\n",
        "            tree_predictions = tree.predict(X)\n",
        "            predictions += tree_predictions\n",
        "\n",
        "        return predictions / self.n_estimators"
      ],
      "metadata": {
        "id": "kfkodnehvE_C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rdf(X, y, feature_types, feature_names, n_estimators=10, max_depth=None, r_features=0.3):\n",
        "    rdf = RandomForest(n_estimators=n_estimators, max_depth=max_depth, r_features=r_features)\n",
        "    rdf.fit(X, y, feature_types, feature_names)\n",
        "    return rdf\n",
        "\n",
        "def predict_rdf(rdf, X):\n",
        "    return rdf.predict(X)"
      ],
      "metadata": {
        "id": "DetW-r7nymDM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results of k-fold cross validation"
      ],
      "metadata": {
        "id": "AH8fxy3Q2dJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rdf = build_rdf(X_train, y_train, feature_types, feature_names, n_estimators=50, r_features=0.5)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = predict_rdf(rdf, X_test)\n",
        "\n",
        "# Compute MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4THo06fL2Yco",
        "outputId": "e806de6f-9409-46c4-f48d-682f8b85d42c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 4.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform k-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "n_trees = 50\n",
        "mse_scores = []\n",
        "r_features = 0.5\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Train the model\n",
        "    rdf = build_rdf(X_train, y_train, feature_types, feature_names, n_estimators=n_trees, r_features=r_features)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_fold = predict_rdf(rdf, X_val)\n",
        "\n",
        "    # Compute MSE\n",
        "    mse = mean_squared_error(y_val, y_pred_fold)\n",
        "    mse_scores.append(mse)\n",
        "\n",
        "    print(f\"MSE for fold: {mse:.2f}\")\n",
        "\n",
        "# Average MSE across all folds on training set\n",
        "avg_mse = sum(mse_scores) / len(mse_scores)\n",
        "print(f\"Average MSE from k-fold Cross-Validation: {avg_mse:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9G2Y51773nA",
        "outputId": "12214109-e7fd-442b-e88a-dcf66a4f6fdf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE for fold: 5.04\n",
            "MSE for fold: 4.41\n",
            "MSE for fold: 5.14\n",
            "MSE for fold: 5.31\n",
            "MSE for fold: 3.80\n",
            "Average MSE from k-fold Cross-Validation: 4.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After assessing the models using k-fold cross-validation, the random forest outperformed the decision tree. The decision tree had an MSE of 8.87, while the random forest achieved a lower MSE of 4.74. This confirms the random forest's advantage in providing more generalized predictions due to its ensemble nature, as evidenced by our experimental findings."
      ],
      "metadata": {
        "id": "f7BvIOhikomu"
      }
    }
  ]
}